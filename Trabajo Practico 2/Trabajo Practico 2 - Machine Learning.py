# -*- coding: utf-8 -*-
"""Tp2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ShMScUlJdw8qZkttiWah7QtwaWJnL_zi
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib
#plt.rcParams['figure.figsize'] = (20, 10)

sns.set(style="whitegrid") # seteando tipo de grid en seaborn

pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs

import warnings
warnings.filterwarnings('ignore')

import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn import tree
from sklearn.preprocessing import OneHotEncoder
!pip install --upgrade scikit-learn
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
id='1Xcx51Afb_Mzw56BCaF0wVu8eGFZWJhcZ'
id2='1F45NhouvIGHdsXLwZ2wU9rZAc16XL8Ls'

downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('train_values.csv')
downloaded2 = drive.CreateFile({'id': id2})
downloaded2.GetContentFile('train_labels.csv')

id3 = "1kKWIYGdPMNp0wv66ZNxAayuWNuh1t7_v"
downloaded3 = drive.CreateFile({'id': id3})
downloaded3.GetContentFile('test_values.csv')
id4 = "1grCPZ3WFSR4T8dRyV8J5CrO_w1psASYT"
downloaded4 = drive.CreateFile({'id': id4})
downloaded4.GetContentFile('submission_format.csv')

"""#train values
These are the features you'll use to train a model. There are 38 features, including structual information such as the number of floors (before the earthquake), age of the building, and type of foundation, as well as legal information such as ownership status, building use, and the number of families who live there. Each building is identified by a unique (random) building_id, which you can use as an index.
"""

train_values = pd.read_csv('train_values.csv', encoding='latin1',sep=',', index_col='building_id')
train_values.head()

"""#train labels
These are the labels. Every building_id in the training values data has a corresponding label in this file. A 1 represents low damage, a 2 represents a medium amount of damage, and a 3 represents almost complete destruction.
"""

train_labels = pd.read_csv('train_labels.csv', encoding='latin1',sep=',', index_col='building_id')

"""#test values
These are the features you'll use to make predictions after training a model. We don't give you the labels for these samples, it's up to you to generate predictions of the level of earthquake damage for these building_ids!
"""

test_values = pd.read_csv('test_values.csv', encoding='latin1',sep=',', index_col='building_id')
test_values

"""#Feature Engeneering

"""

train_labels.value_counts()

"""##.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.

#features a tener en cuenta
"""

selected_features = ["geo_level_1_id","geo_level_2_id","geo_level_3_id","count_floors_pre_eq","area_percentage","height_percentage","has_superstructure_adobe_mud",	"has_superstructure_mud_mortar_stone",	"has_superstructure_stone_flag"	,"has_superstructure_cement_mortar_stone"	,"has_superstructure_mud_mortar_brick",	"has_superstructure_cement_mortar_brick",	"has_superstructure_timber",	"has_superstructure_bamboo",	"has_superstructure_rc_non_engineered",	"has_superstructure_rc_engineered"	,"has_superstructure_other"]
train_values_subset = train_values[selected_features]

train_values_subset = pd.get_dummies(train_values_subset)

"""#Random Forest del modelo

No tiene sentido aplicar transformaciones qe no cambian el orden de los datos
"""

# for preprocessing the data
from sklearn.preprocessing import StandardScaler

# the model
from sklearn.ensemble import RandomForestClassifier

# for combining the preprocess with model training
from sklearn.pipeline import make_pipeline

# for optimizing the hyperparameters of the pipeline
from sklearn.model_selection import GridSearchCV

pipe = make_pipeline(StandardScaler(), 
                     RandomForestClassifier(random_state=2018))
pipe

param_grid = {'randomforestclassifier__n_estimators': [50, 100],
              'randomforestclassifier__min_samples_leaf': [1, 5]}
gs = GridSearchCV(pipe, param_grid, cv=5)

#le lleva como 10 minutos esto
gs.fit(train_values_subset, train_labels.values.ravel())

gs.best_params_

from sklearn.metrics import f1_score

in_sample_preds = gs.predict(train_values_subset)
f1_score(train_labels, in_sample_preds, average='micro')

"""#Predicciones

"""

test_values_subset = test_values[selected_features]
test_values_subset = pd.get_dummies(test_values_subset)


predictions = gs.predict(test_values_subset)

submission_format = pd.read_csv('submission_format.csv', encoding='latin1',sep=',', index_col='building_id')
my_submission = pd.DataFrame(data=predictions,
                             columns=submission_format.columns,
                             index=submission_format.index)

my_submission.head()

my_submission.to_csv('submission.csv')

!head submission.csv

"""#dio 0,72 en el drivendata

##.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.

#Igual pero le sumo las que tenian letras con get_dummies o oneHotEncoding
land_surface_condition	foundation_type	roof_type	ground_floor_type	other_floor_type	position	plan_configuration

#features a tener en cuenta (todos salvo usos)
"""

selected_features = ["geo_level_1_id","geo_level_2_id",	"count_floors_pre_eq"	,"age"	,"area_percentage"	,"height_percentage",	"has_superstructure_adobe_mud"	,"has_superstructure_mud_mortar_stone",	"has_superstructure_stone_flag"	,"has_superstructure_cement_mortar_stone",	"has_superstructure_mud_mortar_brick",	"has_superstructure_cement_mortar_brick",	"has_superstructure_timber"	,"has_superstructure_bamboo"	,"has_superstructure_rc_non_engineered",	"has_superstructure_rc_engineered"	,"has_superstructure_other","land_surface_condition",	"foundation_type",	"roof_type",	"ground_floor_type",	"other_floor_type",	"position"	,"plan_configuration"]
train_values_subset = train_values[selected_features]
train_values_subset["floors/area_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["area_percentage"]
train_values_subset["floors/height_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["height_percentage"]
train_values_subset = pd.get_dummies(train_values_subset)

"""#Random Forest del modelo

No tiene sentido aplicar transformaciones qe no cambian el orden de los datos
"""

# for preprocessing the data
from sklearn.preprocessing import StandardScaler

# the model
from sklearn.ensemble import RandomForestClassifier

# for combining the preprocess with model training
from sklearn.pipeline import make_pipeline

# for optimizing the hyperparameters of the pipeline
from sklearn.model_selection import GridSearchCV

pipe = make_pipeline(StandardScaler(), 
                     RandomForestClassifier(random_state=2018))
pipe

param_grid = {'randomforestclassifier__n_estimators': [50, 150],
              'randomforestclassifier__min_samples_leaf': [1, 5]}
gs = GridSearchCV(pipe, param_grid, cv=5)

#le lleva como 14 minutos esto
gs.fit(train_values_subset, train_labels.values.ravel())

gs.best_params_

from sklearn.metrics import f1_score

in_sample_preds = gs.predict(train_values_subset)
f1_score(train_labels, in_sample_preds, average='micro')

test_values_subset

"""#predicciones"""

selected_features = ["geo_level_1_id","geo_level_2_id",	"count_floors_pre_eq"	,"age"	,"area_percentage"	,"height_percentage",	"has_superstructure_adobe_mud"	,"has_superstructure_mud_mortar_stone",	"has_superstructure_stone_flag"	,"has_superstructure_cement_mortar_stone",	"has_superstructure_mud_mortar_brick",	"has_superstructure_cement_mortar_brick",	"has_superstructure_timber"	,"has_superstructure_bamboo"	,"has_superstructure_rc_non_engineered",	"has_superstructure_rc_engineered"	,"has_superstructure_other","land_surface_condition",	"foundation_type",	"roof_type",	"ground_floor_type",	"other_floor_type",	"position"	,"plan_configuration"]
test_values_subset = test_values[selected_features]
test_values_subset["floors/area_p"] = test_values_subset["count_floors_pre_eq"]/test_values_subset["area_percentage"]
test_values_subset["floors/height_p"] = test_values_subset["count_floors_pre_eq"]/test_values_subset["height_percentage"]
test_values_subset = pd.get_dummies(test_values_subset)


predictions = gs.predict(test_values_subset)

submission_format = pd.read_csv('submission_format.csv', encoding='latin1',sep=',', index_col='building_id')
my_submission = pd.DataFrame(data=predictions,
                             columns=submission_format.columns,
                             index=submission_format.index)

my_submission.head()

my_submission.to_csv('submission.csv')

!head submission.csv

"""#0.71 en driven data, debe ser overfitting. f1 dio casi 1 además

#En el proximo habria que empezar a jugar con la interaccion entre features (lo que hicimos en el tp1), es importante en arboles porque no tienen en cuenta esto. Splitean por columna de manera aislada
"""

forest = RandomForestClassifier(random_state = 123, n_estimators = 100, min_samples_leaf = 1)
X=train_values_subset
y=train_labels.values.ravel()
forest.fit(X,y)

#es lo mismo que arriba, pero es para ver los features
from sklearn.metrics import f1_score

in_sample_preds = forest.predict(train_values_subset)
f1_score(train_labels, in_sample_preds, average='micro')

#mas importantes geo1 geo2 geo3 age(no filtre los casos raros) area_percentage height_percentage has_superstructure_adobe_mud
plt.bar(train_values_subset.columns,forest.feature_importances_)
plt.xlabel("features")
plt.ylabel("Importance")
plt.title("feature Importance en Random Forest")
plt.rcParams["figure.figsize"] = (80,30)
plt.show()

"""#habria que probar balanceando la cantidad de edificios con daño 1,2 y 3

##.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.
#.

#XGBoost
"""

#instancio el regresor
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV

xgb_reg = xgb.XGBClassifier(objetive="binary:logistic",seed = 123)

"""#45 min

HalvingGridSearchCV(cv=3,

                    estimator=XGBClassifier(objetive='binary:logistic',

                                            seed=123),

                    n_jobs=-1,

                    param_grid={'base_score': [0.5],

                                'colsample_bytree': [0.4, 0.6],

                                'learning_rate': [0.1, 0.2],

                                'max_delta_step': [0], 'max_depth': [4, 7, 9],

                                'n_estimators': [100, 200],

                                'subsample': [0.7, 0.8]},

                    refit=<function _refit_callable at 0x7f68b05b4440>,

                    scoring='f1_micro')


  #best: {'base_score': 0.5,
 #'colsample_bytree': 0.6,
 #'learning_rate': 0.1,
 #'max_delta_step': 0,
 #'max_depth': 9,
 #'n_estimators': 100,
 #'subsample': 0.8}
 #0.7281134283686038, 0.7267 en drivendata

XGB

{'base_score': 0.5,
 'colsample_bytree': 0.6,
 'learning_rate': 0.1,
 'max_delta_step': 0,
 'max_depth': 13,
 'n_estimators': 100,
 'subsample': 0.8}  llevo 1:53


 quedaron {'base_score': 0.5,
 'colsample_bytree': 0.6,
 'learning_rate': 0.1,
 'max_delta_step': 0,
 'max_depth': 13,
 'n_estimators': 100,
 'subsample': 0.8}

 cv = 5


 F1:  0.7408722012240747
 drivenData: 0.7396
"""

parameters =    {
            'max_depth': [9, 13],
            'learning_rate': [0.1, 0.2],
            'n_estimators': [100,150],
            'max_delta_step': [0],
            'subsample': [ 0.8,0.9],
            'colsample_bytree': [ 0.6,0.7],
            'base_score': [0.5],
            }




#parameters =    {
#            'max_depth': [3, 4, 5,7,9],
#            'learning_rate': [0.1, 0.2, 0.3, 0.4],
#            'n_estimators': [50, 100, 150, 200],
#            'gamma': [0, 0.1, 0.2],
#            'min_child_weight': [0, 0.5, 1],
#            'max_delta_step': [0],
#            'subsample': [0.7, 0.8, 0.9, 1],
#            'colsample_bytree': [0.4, 0.6, 0.8, 1],
#            'colsample_bylevel': [1],
#            'reg_alpha': [0, 1e-2, 1, 1e1, 10],
#            'reg_lambda': [0, 1e-2, 1, 1e1],
#            'base_score': [0.5],
#
#            }

HGS = HalvingGridSearchCV(xgb_reg, parameters, scoring = 'f1_micro', n_jobs=-1, cv=5)

selected_features = ["geo_level_1_id","geo_level_2_id","geo_level_3_id",	"count_floors_pre_eq"	,"age"	,"area_percentage"	,"height_percentage",	"has_superstructure_adobe_mud"	,"has_superstructure_mud_mortar_stone",	"has_superstructure_stone_flag"	,"has_superstructure_cement_mortar_stone",	"has_superstructure_mud_mortar_brick",	"has_superstructure_cement_mortar_brick",	"has_superstructure_timber"	,"has_superstructure_bamboo"	,"has_superstructure_rc_non_engineered",	"has_superstructure_rc_engineered"	,"has_superstructure_other","land_surface_condition",	"foundation_type",	"roof_type",	"ground_floor_type",	"other_floor_type",	"position"	,"plan_configuration"]
train_values_subset = train_values[selected_features]
train_values_subset["floors/area_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["area_percentage"]
train_values_subset["floors/height_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["height_percentage"]
train_values_subset["no_secondary_use"] = 1 if  train_values_subset["has_secondary_use"] == 0 else 0
train_values_subset = pd.get_dummies(train_values_subset)

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(train_values_subset,train_labels,test_size=0.2,train_size=0.8)

"""#entreno"""

HGS.fit(X_train,y_train)

HGS.best_params_

preds = HGS.predict(X_test)

"""#error f1"""

from sklearn.metrics import f1_score


f1_score(y_test, preds, average='micro')

"""#prediccion"""

test_values_subset = test_values[selected_features]
test_values_subset["floors/area_p"] = test_values_subset["count_floors_pre_eq"]/test_values_subset["area_percentage"]
test_values_subset["floors/height_p"] = test_values_subset["count_floors_pre_eq"]/test_values_subset["height_percentage"]
test_values_subset = pd.get_dummies(test_values_subset)


predictions = HGS.predict(test_values_subset)

submission_format = pd.read_csv('submission_format.csv', encoding='latin1',sep=',', index_col='building_id')
my_submission = pd.DataFrame(data=predictions,
                             columns=submission_format.columns,
                             index=submission_format.index)

my_submission.head()

my_submission.to_csv('submission.csv')
!head submission.csv

"""#XGB cronologicamente posterior a anteriores

"""

#instancio el regresor
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV

xgb_reg = xgb.XGBClassifier(objetive="multi:softprob",seed = 123)

parameters = {'base_score':[0.5] ,
 'colsample_bytree': [0.6],
 'learning_rate': [0.1],
 'max_delta_step': [1],
 'max_depth': [13],
 'n_estimators': [150],
 'subsample': [0.8]}

HGS = HalvingGridSearchCV(xgb_reg, parameters, scoring = 'f1_micro', n_jobs=-1, cv=5)

"""columna uses arbol

"""

xgb_clas_use = xgb.XGBClassifier(objetive="multi:softprob",seed = 123)

parameters_xgb_clas_use = {'base_score':[0.5] ,
 'colsample_bytree': [0.6],
 'learning_rate': [0.1],
 'max_delta_step': [1],
 'max_depth': [25],
 'n_estimators': [60],
 'subsample': [0.2]}

HGS_xgb_clas_use = HalvingGridSearchCV(xgb_clas_use, parameters_xgb_clas_use, scoring = 'f1_micro', n_jobs=-1, cv=5)

selected_features_xgb_clas_use = ["has_secondary_use_agriculture"	,"has_secondary_use_hotel",	"has_secondary_use_rental",	"has_secondary_use_institution"	,"has_secondary_use_school",	"has_secondary_use_industry"	,"has_secondary_use_health_post"	,"has_secondary_use_gov_office",	"has_secondary_use_use_police"	,"has_secondary_use_other"]
train_values_subset_xgb_clas_use = train_values[selected_features_xgb_clas_use]

from sklearn.model_selection import train_test_split
X_train_u,X_test_u,y_train_u,y_test_u = train_test_split(train_values_subset_xgb_clas_use,train_labels,test_size=0.2,train_size=0.8)

HGS_xgb_clas_use.fit(X_train_u,y_train_u)

HGS_xgb_clas_use.best_params_

preds_xgb_clas_use = HGS_xgb_clas_use.predict(X_test_u)
from sklearn.metrics import f1_score




f1_score(y_test_u, preds_xgb_clas_use, average='micro')

"""fin col uses arbol

Columna testeando geoLevel arbol
"""

#instancio el clasificador


xgb_clas = xgb.XGBClassifier(objetive="multi:softprob",seed = 123)

parameters_xgb_clas = {'base_score':[0.5] ,
 'colsample_bytree': [0.6],
 'learning_rate': [0.1],
 'max_delta_step': [1],
 'max_depth': [13],
 'n_estimators': [300],
 'subsample': [0.2]}

HGS_xgb_clas = HalvingGridSearchCV(xgb_clas, parameters_xgb_clas, scoring = 'f1_micro', n_jobs=-1, cv=5)

selected_features_xgb_clas = ["geo_level_1_id","geo_level_2_id","geo_level_3_id"]
train_values_subset_xgb_clas = train_values[selected_features_xgb_clas]

from sklearn.model_selection import train_test_split
X_train_,X_test_,y_train_,y_test_ = train_test_split(train_values_subset_xgb_clas,train_labels,test_size=0.2,train_size=0.8)

HGS_xgb_clas.fit(X_train_,y_train_)

HGS_xgb_clas.best_params_

preds_xgb_clas = HGS_xgb_clas.predict(X_test_)
from sklearn.metrics import f1_score


f1_score(y_test_, preds_xgb_clas, average='micro')

preds_xgb_clas

"""Fin geoLevelColumna

"""

from scipy.stats.mstats import winsorize

selected_features = ["geo_level_1_id","geo_level_2_id","geo_level_3_id",	"count_floors_pre_eq"	,"age"	,"area_percentage"	,"height_percentage",	"has_superstructure_adobe_mud"	,"has_superstructure_mud_mortar_stone",	"has_superstructure_stone_flag"	,"has_superstructure_cement_mortar_stone",	"has_superstructure_mud_mortar_brick",	"has_superstructure_cement_mortar_brick",	"has_superstructure_timber"	,"has_superstructure_bamboo"	,"has_superstructure_rc_non_engineered",	"has_superstructure_rc_engineered"	,"has_superstructure_other","land_surface_condition",	"foundation_type",	"roof_type",	"ground_floor_type",	"other_floor_type",	"position"	,"plan_configuration","has_secondary_use",	"has_secondary_use_agriculture"	,"has_secondary_use_hotel",	"has_secondary_use_rental",	"has_secondary_use_institution"	,"has_secondary_use_school",	"has_secondary_use_industry"	,"has_secondary_use_health_post"	,"has_secondary_use_gov_office",	"has_secondary_use_use_police"	,"has_secondary_use_other"]

#selected_features = ["count_floors_pre_eq"	,"age"	,"area_percentage"	,"height_percentage",	"has_superstructure_adobe_mud"	,"has_superstructure_mud_mortar_stone",	"has_superstructure_stone_flag"	,"has_superstructure_cement_mortar_stone",	"has_superstructure_mud_mortar_brick",	"has_superstructure_cement_mortar_brick",	"has_superstructure_timber"	,"has_superstructure_bamboo"	,"has_superstructure_rc_non_engineered",	"has_superstructure_rc_engineered"	,"has_superstructure_other","land_surface_condition",	"foundation_type",	"roof_type",	"ground_floor_type",	"other_floor_type",	"position"	,"plan_configuration"]
train_values_subset = train_values[selected_features]


train_values_subset["damage_use"]= HGS_xgb_clas_use.predict(train_values_subset_xgb_clas_use)
train_values_subset["damage_geo_level"] = HGS_xgb_clas.predict(train_values[["geo_level_1_id","geo_level_2_id","geo_level_3_id"]])







train_values_subset["age"] = winsorize(train_values_subset["age"],(0, 0.05))
train_values_subset["area_percentage"] = winsorize(train_values_subset["area_percentage"],(0, 0.05))
train_values_subset["height_percentage"] = winsorize(train_values_subset["height_percentage"],(0, 0.05))



train_values_subset["floors/area_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["area_percentage"]
train_values_subset["floors/height_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["height_percentage"]
train_values_subset["height_p/area_p"] = train_values_subset["height_percentage"]/train_values_subset["area_percentage"]
train_values_subset["doesnt_have_secondary_use"] = 1-train_values_subset["has_secondary_use"]
del train_values_subset["has_secondary_use"]
train_values_subset["has_secondary_use_other"]  = train_values_subset["has_secondary_use_other"]  |  train_values_subset["has_secondary_use_rental"] |  train_values_subset["has_secondary_use_use_police"] |  train_values_subset["has_secondary_use_institution"]|  train_values_subset["has_secondary_use_school"]|  train_values_subset["has_secondary_use_industry"]|  train_values_subset["has_secondary_use_health_post"]|  train_values_subset["has_secondary_use_gov_office"]
del train_values_subset["has_secondary_use_rental"]
del train_values_subset["has_secondary_use_institution"]
del train_values_subset["has_secondary_use_industry"]
del train_values_subset["has_secondary_use_health_post"]
del train_values_subset["has_secondary_use_gov_office"]
del train_values_subset["has_secondary_use_use_police"]
del train_values_subset["has_secondary_use_school"]




train_values_subset = pd.get_dummies(train_values_subset)
train_values_subset.head()





#0.7434623280443583 el f1 0.7411 en driven data (con multisoft prob dio 7416 en drivendata)
#selected_features = ["geo_level_1_id","geo_level_2_id","geo_level_3_id",	"count_floors_pre_eq"	,"age"	,"area_percentage"	,"height_percentage",	"has_superstructure_adobe_mud"	,"has_superstructure_mud_mortar_stone",	"has_superstructure_stone_flag"	,"has_superstructure_cement_mortar_stone",	"has_superstructure_mud_mortar_brick",	"has_superstructure_cement_mortar_brick",	"has_superstructure_timber"	,"has_superstructure_bamboo"	,"has_superstructure_rc_non_engineered",	"has_superstructure_rc_engineered"	,"has_superstructure_other","land_surface_condition",	"foundation_type",	"roof_type",	"ground_floor_type",	"other_floor_type",	"position"	,"plan_configuration","has_secondary_use",	"has_secondary_use_agriculture"	,"has_secondary_use_hotel",	"has_secondary_use_rental",	"has_secondary_use_institution"	,"has_secondary_use_school",	"has_secondary_use_industry"	,"has_secondary_use_health_post"	,"has_secondary_use_gov_office",	"has_secondary_use_use_police"	,"has_secondary_use_other"]
#train_values_subset = train_values[selected_features]
#train_values_subset["floors/area_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["area_percentage"]
#train_values_subset["floors/height_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["height_percentage"]
#train_values_subset["height_p/area_p"] = train_values_subset["height_percentage"]/train_values_subset["area_percentage"]
#train_values_subset["doesnt_have_secondary_use"] = 1-train_values_subset["has_secondary_use"]
#del train_values_subset["has_secondary_use"]
#train_values_subset["has_secondary_use_other"]  = train_values_subset["has_secondary_use_other"]  |  train_values_subset["has_secondary_use_rental"] |  train_values_subset["has_secondary_use_use_police"] |  train_values_subset["has_secondary_use_institution"]|  train_values_subset["has_secondary_use_school"]|  train_values_subset["has_secondary_use_industry"]|  train_values_subset["has_secondary_use_health_post"]|  train_values_subset["has_secondary_use_gov_office"]
#del train_values_subset["has_secondary_use_rental"]
#del train_values_subset["has_secondary_use_institution"]
#del train_values_subset["has_secondary_use_industry"]
#del train_values_subset["has_secondary_use_health_post"]
#del train_values_subset["has_secondary_use_gov_office"]
#del train_values_subset["has_secondary_use_use_police"]
#el train_values_subset["has_secondary_use_school"]
#train_values_subset = pd.get_dummies(train_values_subset)
#train_values_subset.head()

#con lo de abajo dio 0.7431553500508432 f1 y en driven data 0.7417
#selected_features = ["geo_level_1_id","geo_level_2_id","geo_level_3_id",	"count_floors_pre_eq"	,"age"	,"area_percentage"	,"height_percentage",	"has_superstructure_adobe_mud"	,"has_superstructure_mud_mortar_stone",	"has_superstructure_stone_flag"	,"has_superstructure_cement_mortar_stone",	"has_superstructure_mud_mortar_brick",	"has_superstructure_cement_mortar_brick",	"has_superstructure_timber"	,"has_superstructure_bamboo"	,"has_superstructure_rc_non_engineered",	"has_superstructure_rc_engineered"	,"has_superstructure_other","land_surface_condition",	"foundation_type",	"roof_type",	"ground_floor_type",	"other_floor_type",	"position"	,"plan_configuration","has_secondary_use",	"has_secondary_use_agriculture"	,"has_secondary_use_hotel",	"has_secondary_use_rental",	"has_secondary_use_institution"	,"has_secondary_use_school",	"has_secondary_use_industry"	,"has_secondary_use_health_post"	,"has_secondary_use_gov_office",	"has_secondary_use_use_police"	,"has_secondary_use_other"]
#train_values_subset = train_values[selected_features]
#train_values_subset["floors/area_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["area_percentage"]
#train_values_subset["floors/height_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["height_percentage"]

#train_values_subset["doesnt_have_secondary_use"] = 1-train_values_subset["has_secondary_use"]
#del train_values_subset["has_secondary_use"]
#train_values_subset["has_secondary_use_other"]  = train_values_subset["has_secondary_use_other"]  |  train_values_subset["has_secondary_use_rental"] |  train_values_subset["has_secondary_use_use_police"] |  train_values_subset["has_secondary_use_institution"]|  train_values_subset["has_secondary_use_school"]|  train_values_subset["has_secondary_use_industry"]|  train_values_subset["has_secondary_use_health_post"]|  train_values_subset["has_secondary_use_gov_office"]
#del train_values_subset["has_secondary_use_rental"]
#del train_values_subset["has_secondary_use_institution"]
#del train_values_subset["has_secondary_use_industry"]
#del train_values_subset["has_secondary_use_health_post"]
#del train_values_subset["has_secondary_use_gov_office"]
##del train_values_subset["has_secondary_use_use_police"]
#del train_values_subset["has_secondary_use_school"]
#train_values_subset = pd.get_dummies(train_values_subset)
#train_values_subset.head()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(train_values_subset,train_labels,test_size=0.2,train_size=0.8)

HGS.fit(X_train,y_train)

HGS.best_params_

preds = HGS.predict(X_test)

from sklearn.metrics import f1_score


f1_score(y_test, preds, average='micro')

test_values_subset = test_values[selected_features]
test_uses = test_values[["has_secondary_use_agriculture"	,"has_secondary_use_hotel",	"has_secondary_use_rental",	"has_secondary_use_institution"	,"has_secondary_use_school",	"has_secondary_use_industry"	,"has_secondary_use_health_post"	,"has_secondary_use_gov_office",	"has_secondary_use_use_police"	,"has_secondary_use_other"]]

test_values_subset["damage_use"]= HGS_xgb_clas_use.predict(test_uses)
test_values_subset["damage_geo_level"] = HGS_xgb_clas.predict(test_values[["geo_level_1_id","geo_level_2_id","geo_level_3_id"]])




test_values_subset["age"] = winsorize(test_values_subset["age"],(0, 0.05))
test_values_subset["area_percentage"] = winsorize(test_values_subset["area_percentage"],(0, 0.055))
test_values_subset["height_percentage"] = winsorize(test_values_subset["height_percentage"],(0, 0.04))


test_values_subset["floors/area_p"] = test_values_subset["count_floors_pre_eq"]/test_values_subset["area_percentage"]
test_values_subset["floors/height_p"] = test_values_subset["count_floors_pre_eq"]/test_values_subset["height_percentage"]
test_values_subset["height_p/area_p"] = test_values_subset["height_percentage"]/test_values_subset["area_percentage"]


test_values_subset["doesnt_have_secondary_use"] = 1-test_values_subset["has_secondary_use"]
del test_values_subset["has_secondary_use"]
test_values_subset["has_secondary_use_other"]  = test_values_subset["has_secondary_use_other"]  |  test_values_subset["has_secondary_use_rental"] |  test_values_subset["has_secondary_use_use_police"] |  test_values_subset["has_secondary_use_institution"]|  test_values_subset["has_secondary_use_school"]|  test_values_subset["has_secondary_use_industry"]|  test_values_subset["has_secondary_use_health_post"]|  test_values_subset["has_secondary_use_gov_office"]
del test_values_subset["has_secondary_use_rental"]
del test_values_subset["has_secondary_use_institution"]
del test_values_subset["has_secondary_use_industry"]
del test_values_subset["has_secondary_use_health_post"]
del test_values_subset["has_secondary_use_gov_office"]
del test_values_subset["has_secondary_use_use_police"]
del test_values_subset["has_secondary_use_school"]

test_values_subset = pd.get_dummies(test_values_subset)


predictions = HGS.predict(test_values_subset)

submission_format = pd.read_csv('submission_format.csv', encoding='latin1',sep=',', index_col='building_id')
my_submission = pd.DataFrame(data=predictions,
                             columns=submission_format.columns,
                             index=submission_format.index)

my_submission.head()

my_submission.to_csv('submission.csv')
!head submission.csv

from google.colab import files
files.download("submission.csv")

"""#XGB Softmax Sin geo id, F1 baja a 0.61


"""

#instancio el regresor
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV

xgb_reg = xgb.XGBClassifier(objetive="multi:softmax",seed = 123)

parameters = {'base_score':[0.5] ,
 'colsample_bytree': [0.6],
 'learning_rate': [0.1],
 'max_delta_step': [1],
 'max_depth': [13,20,25],
 'n_estimators': [120],
 'subsample': [0.8]}

HGS = HalvingGridSearchCV(xgb_reg, parameters, scoring = 'f1_micro', n_jobs=-1, cv=5)

from scipy.stats.mstats import winsorize

selected_features = [	"count_floors_pre_eq"	,"age"	,"area_percentage"	,"height_percentage",	"has_superstructure_adobe_mud"	,"has_superstructure_mud_mortar_stone",	"has_superstructure_stone_flag"	,"has_superstructure_cement_mortar_stone",	"has_superstructure_mud_mortar_brick",	"has_superstructure_cement_mortar_brick",	"has_superstructure_timber"	,"has_superstructure_bamboo"	,"has_superstructure_rc_non_engineered",	"has_superstructure_rc_engineered"	,"has_superstructure_other","land_surface_condition",	"foundation_type",	"roof_type",	"ground_floor_type",	"other_floor_type",	"position"	,"plan_configuration","has_secondary_use",	"has_secondary_use_agriculture"	,"has_secondary_use_hotel",	"has_secondary_use_rental",	"has_secondary_use_institution"	,"has_secondary_use_school",	"has_secondary_use_industry"	,"has_secondary_use_health_post"	,"has_secondary_use_gov_office",	"has_secondary_use_use_police"	,"has_secondary_use_other"]
train_values_subset = train_values[selected_features]

train_values_subset["age"] = winsorize(train_values_subset["age"],(0, 0.05))
train_values_subset["area_percentage"] = winsorize(train_values_subset["area_percentage"],(0, 0.055))
train_values_subset["height_percentage"] = winsorize(train_values_subset["height_percentage"],(0, 0.04))



train_values_subset["floors/area_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["area_percentage"]
train_values_subset["floors/height_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["height_percentage"]
train_values_subset["height_p/area_p"] = train_values_subset["height_percentage"]/train_values_subset["area_percentage"]
train_values_subset["doesnt_have_secondary_use"] = 1-train_values_subset["has_secondary_use"]
del train_values_subset["has_secondary_use"]
train_values_subset["has_secondary_use_other"]  = train_values_subset["has_secondary_use_other"]  |  train_values_subset["has_secondary_use_rental"] |  train_values_subset["has_secondary_use_use_police"] |  train_values_subset["has_secondary_use_institution"]|  train_values_subset["has_secondary_use_school"]|  train_values_subset["has_secondary_use_industry"]|  train_values_subset["has_secondary_use_health_post"]|  train_values_subset["has_secondary_use_gov_office"]
del train_values_subset["has_secondary_use_rental"]
del train_values_subset["has_secondary_use_institution"]
del train_values_subset["has_secondary_use_industry"]
del train_values_subset["has_secondary_use_health_post"]
del train_values_subset["has_secondary_use_gov_office"]
del train_values_subset["has_secondary_use_use_police"]
del train_values_subset["has_secondary_use_school"]

train_values_subset = pd.get_dummies(train_values_subset)
train_values_subset.head()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(train_values_subset,train_labels,test_size=0.2,train_size=0.8)

HGS.fit(X_train,y_train)

HGS.best_params_

preds = HGS.predict(X_test)

from sklearn.metrics import f1_score
f1_score(y_test, preds, average='micro')

"""#XGB agrego feature age*floors"""

#instancio el regresor
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV

xgb_reg = xgb.XGBClassifier(objetive="multi:softprob",seed = 123)

parameters = {'base_score':[0.5] ,
 'colsample_bytree': [0.6],
 'learning_rate': [0.1],
 'max_delta_step': [1],
 'max_depth': [13],
 'n_estimators': [120],
 'subsample': [0.8]}

HGS = HalvingGridSearchCV(xgb_reg, parameters, scoring = 'f1_micro', n_jobs=-1, cv=5)

from scipy.stats.mstats import winsorize

selected_features = ["geo_level_1_id","geo_level_2_id","geo_level_3_id",	"count_floors_pre_eq"	,"age"	,"area_percentage"	,"height_percentage",	"has_superstructure_adobe_mud"	,"has_superstructure_mud_mortar_stone",	"has_superstructure_stone_flag"	,"has_superstructure_cement_mortar_stone",	"has_superstructure_mud_mortar_brick",	"has_superstructure_cement_mortar_brick",	"has_superstructure_timber"	,"has_superstructure_bamboo"	,"has_superstructure_rc_non_engineered",	"has_superstructure_rc_engineered"	,"has_superstructure_other","land_surface_condition",	"foundation_type",	"roof_type",	"ground_floor_type",	"other_floor_type",	"position"	,"plan_configuration","has_secondary_use",	"has_secondary_use_agriculture"	,"has_secondary_use_hotel",	"has_secondary_use_rental",	"has_secondary_use_institution"	,"has_secondary_use_school",	"has_secondary_use_industry"	,"has_secondary_use_health_post"	,"has_secondary_use_gov_office",	"has_secondary_use_use_police"	,"has_secondary_use_other"]
train_values_subset = train_values[selected_features]

train_values_subset["age"] = winsorize(train_values_subset["age"],(0, 0.05))
train_values_subset["area_percentage"] = winsorize(train_values_subset["area_percentage"],(0, 0.055))
train_values_subset["height_percentage"] = winsorize(train_values_subset["height_percentage"],(0, 0.04))



train_values_subset["floors/area_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["area_percentage"]
train_values_subset["floors/height_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["height_percentage"]
train_values_subset["height_p/area_p"] = train_values_subset["height_percentage"]/train_values_subset["area_percentage"]
train_values_subset["age*floors"] = train_values_subset["age"]*train_values_subset["count_floors_pre_eq"]


train_values_subset["doesnt_have_secondary_use"] = 1-train_values_subset["has_secondary_use"]
del train_values_subset["has_secondary_use"]
train_values_subset["has_secondary_use_other"]  = train_values_subset["has_secondary_use_other"]  |  train_values_subset["has_secondary_use_rental"] |  train_values_subset["has_secondary_use_use_police"] |  train_values_subset["has_secondary_use_institution"]|  train_values_subset["has_secondary_use_school"]|  train_values_subset["has_secondary_use_industry"]|  train_values_subset["has_secondary_use_health_post"]|  train_values_subset["has_secondary_use_gov_office"]
del train_values_subset["has_secondary_use_rental"]
del train_values_subset["has_secondary_use_institution"]
del train_values_subset["has_secondary_use_industry"]
del train_values_subset["has_secondary_use_health_post"]
del train_values_subset["has_secondary_use_gov_office"]
del train_values_subset["has_secondary_use_use_police"]
del train_values_subset["has_secondary_use_school"]

train_values_subset = pd.get_dummies(train_values_subset)
train_values_subset.head()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(train_values_subset,train_labels,test_size=0.2,train_size=0.8)

HGS.fit(X_train,y_train)

HGS.best_params_

preds = HGS.predict(X_test)

from sklearn.metrics import f1_score


f1_score(y_test, preds, average='micro')

test_values_subset = test_values[selected_features]


test_values_subset["age"] = winsorize(test_values_subset["age"],(0, 0.05))
test_values_subset["area_percentage"] = winsorize(test_values_subset["area_percentage"],(0, 0.055))
test_values_subset["height_percentage"] = winsorize(test_values_subset["height_percentage"],(0, 0.04))


test_values_subset["floors/area_p"] = test_values_subset["count_floors_pre_eq"]/test_values_subset["area_percentage"]
test_values_subset["floors/height_p"] = test_values_subset["count_floors_pre_eq"]/test_values_subset["height_percentage"]
test_values_subset["height_p/area_p"] = test_values_subset["height_percentage"]/test_values_subset["area_percentage"]
test_values_subset["age*floors"] = test_values_subset["age"]*test_values_subset["count_floors_pre_eq"]

test_values_subset["doesnt_have_secondary_use"] = 1-test_values_subset["has_secondary_use"]
del test_values_subset["has_secondary_use"]
test_values_subset["has_secondary_use_other"]  = test_values_subset["has_secondary_use_other"]  |  test_values_subset["has_secondary_use_rental"] |  test_values_subset["has_secondary_use_use_police"] |  test_values_subset["has_secondary_use_institution"]|  test_values_subset["has_secondary_use_school"]|  test_values_subset["has_secondary_use_industry"]|  test_values_subset["has_secondary_use_health_post"]|  test_values_subset["has_secondary_use_gov_office"]
del test_values_subset["has_secondary_use_rental"]
del test_values_subset["has_secondary_use_institution"]
del test_values_subset["has_secondary_use_industry"]
del test_values_subset["has_secondary_use_health_post"]
del test_values_subset["has_secondary_use_gov_office"]
del test_values_subset["has_secondary_use_use_police"]
del test_values_subset["has_secondary_use_school"]

test_values_subset = pd.get_dummies(test_values_subset)


predictions = HGS.predict(test_values_subset)

submission_format = pd.read_csv('submission_format.csv', encoding='latin1',sep=',', index_col='building_id')
my_submission = pd.DataFrame(data=predictions,
                             columns=submission_format.columns,
                             index=submission_format.index)

my_submission.head()

my_submission.to_csv('submission.csv')
!head submission.csv

from google.colab import files
files.download("submission.csv")

"""#XGB agrego 150 estimadores"""

#instancio el regresor
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV

xgb_reg = xgb.XGBClassifier(objetive="multi:softprob",seed = 123)

parameters = {'base_score':[0.5] ,
 'colsample_bytree': [0.6],
 'learning_rate': [0.1],
 'max_delta_step': [1],
 'max_depth': [13],
 'n_estimators': [150],
 'subsample': [0.8]}

HGS = HalvingGridSearchCV(xgb_reg, parameters, scoring = 'f1_micro', n_jobs=-1, cv=5)

from scipy.stats.mstats import winsorize

selected_features = ["geo_level_1_id","geo_level_2_id","geo_level_3_id",	"count_floors_pre_eq"	,"age"	,"area_percentage"	,"height_percentage",	"has_superstructure_adobe_mud"	,"has_superstructure_mud_mortar_stone",	"has_superstructure_stone_flag"	,"has_superstructure_cement_mortar_stone",	"has_superstructure_mud_mortar_brick",	"has_superstructure_cement_mortar_brick",	"has_superstructure_timber"	,"has_superstructure_bamboo"	,"has_superstructure_rc_non_engineered",	"has_superstructure_rc_engineered"	,"has_superstructure_other","land_surface_condition",	"foundation_type",	"roof_type",	"ground_floor_type",	"other_floor_type",	"position"	,"plan_configuration","has_secondary_use",	"has_secondary_use_agriculture"	,"has_secondary_use_hotel",	"has_secondary_use_rental",	"has_secondary_use_institution"	,"has_secondary_use_school",	"has_secondary_use_industry"	,"has_secondary_use_health_post"	,"has_secondary_use_gov_office",	"has_secondary_use_use_police"	,"has_secondary_use_other"]
train_values_subset = train_values[selected_features]

train_values_subset["age"] = winsorize(train_values_subset["age"],(0, 0.05))
train_values_subset["area_percentage"] = winsorize(train_values_subset["area_percentage"],(0, 0.055))
train_values_subset["height_percentage"] = winsorize(train_values_subset["height_percentage"],(0, 0.04))



train_values_subset["floors/area_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["area_percentage"]
train_values_subset["floors/height_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["height_percentage"]
train_values_subset["height_p/area_p"] = train_values_subset["height_percentage"]/train_values_subset["area_percentage"]
train_values_subset["age*floors"] = train_values_subset["age"]*train_values_subset["count_floors_pre_eq"]


train_values_subset["doesnt_have_secondary_use"] = 1-train_values_subset["has_secondary_use"]
del train_values_subset["has_secondary_use"]
train_values_subset["has_secondary_use_other"]  = train_values_subset["has_secondary_use_other"]  |  train_values_subset["has_secondary_use_rental"] |  train_values_subset["has_secondary_use_use_police"] |  train_values_subset["has_secondary_use_institution"]|  train_values_subset["has_secondary_use_school"]|  train_values_subset["has_secondary_use_industry"]|  train_values_subset["has_secondary_use_health_post"]|  train_values_subset["has_secondary_use_gov_office"]
del train_values_subset["has_secondary_use_rental"]
del train_values_subset["has_secondary_use_institution"]
del train_values_subset["has_secondary_use_industry"]
del train_values_subset["has_secondary_use_health_post"]
del train_values_subset["has_secondary_use_gov_office"]
del train_values_subset["has_secondary_use_use_police"]
del train_values_subset["has_secondary_use_school"]

train_values_subset = pd.get_dummies(train_values_subset)
train_values_subset.head()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(train_values_subset,train_labels,test_size=0.2,train_size=0.8)

HGS.fit(X_train,y_train)

HGS.best_params_

preds = HGS.predict(X_test)

from sklearn.metrics import f1_score


f1_score(y_test, preds, average='micro')

test_values_subset = test_values[selected_features]


test_values_subset["age"] = winsorize(test_values_subset["age"],(0, 0.05))
test_values_subset["area_percentage"] = winsorize(test_values_subset["area_percentage"],(0, 0.055))
test_values_subset["height_percentage"] = winsorize(test_values_subset["height_percentage"],(0, 0.04))


test_values_subset["floors/area_p"] = test_values_subset["count_floors_pre_eq"]/test_values_subset["area_percentage"]
test_values_subset["floors/height_p"] = test_values_subset["count_floors_pre_eq"]/test_values_subset["height_percentage"]
test_values_subset["height_p/area_p"] = test_values_subset["height_percentage"]/test_values_subset["area_percentage"]
test_values_subset["age*floors"] = test_values_subset["age"]*test_values_subset["count_floors_pre_eq"]

test_values_subset["doesnt_have_secondary_use"] = 1-test_values_subset["has_secondary_use"]
del test_values_subset["has_secondary_use"]
test_values_subset["has_secondary_use_other"]  = test_values_subset["has_secondary_use_other"]  |  test_values_subset["has_secondary_use_rental"] |  test_values_subset["has_secondary_use_use_police"] |  test_values_subset["has_secondary_use_institution"]|  test_values_subset["has_secondary_use_school"]|  test_values_subset["has_secondary_use_industry"]|  test_values_subset["has_secondary_use_health_post"]|  test_values_subset["has_secondary_use_gov_office"]
del test_values_subset["has_secondary_use_rental"]
del test_values_subset["has_secondary_use_institution"]
del test_values_subset["has_secondary_use_industry"]
del test_values_subset["has_secondary_use_health_post"]
del test_values_subset["has_secondary_use_gov_office"]
del test_values_subset["has_secondary_use_use_police"]
del test_values_subset["has_secondary_use_school"]

test_values_subset = pd.get_dummies(test_values_subset)


predictions = HGS.predict(test_values_subset)

submission_format = pd.read_csv('submission_format.csv', encoding='latin1',sep=',', index_col='building_id')
my_submission = pd.DataFrame(data=predictions,
                             columns=submission_format.columns,
                             index=submission_format.index)

my_submission.head()

my_submission.to_csv('submission.csv')
!head submission.csv

from google.colab import files
files.download("submission.csv")

#instancio el regresor
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV

xgb_reg = xgb.XGBClassifier(objetive="multi:softprob",seed = 123)

parameters = {'base_score':[0.5] ,
 'colsample_bytree': [0.6],
 'learning_rate': [0.1],
 'max_delta_step': [1],
 'max_depth': [10,13],
 'n_estimators': [100],
 'subsample': [0.8]}

HGS = HalvingGridSearchCV(xgb_reg, parameters, scoring = 'f1_micro', n_jobs=-1, cv=5)

from scipy.stats.mstats import winsorize

selected_features = ["geo_level_1_id","geo_level_2_id","geo_level_3_id",	"count_floors_pre_eq"	,"age"	,"area_percentage"	,"height_percentage",	"has_superstructure_adobe_mud"	,"has_superstructure_mud_mortar_stone",	"has_superstructure_stone_flag"	,"has_superstructure_cement_mortar_stone",	"has_superstructure_mud_mortar_brick",	"has_superstructure_cement_mortar_brick",	"has_superstructure_timber"	,"has_superstructure_bamboo"	,"has_superstructure_rc_non_engineered",	"has_superstructure_rc_engineered"	,"has_superstructure_other","land_surface_condition",	"foundation_type",	"roof_type",	"ground_floor_type",	"other_floor_type",	"position"	,"plan_configuration","has_secondary_use",	"has_secondary_use_agriculture"	,"has_secondary_use_hotel",	"has_secondary_use_rental",	"has_secondary_use_institution"	,"has_secondary_use_school",	"has_secondary_use_industry"	,"has_secondary_use_health_post"	,"has_secondary_use_gov_office",	"has_secondary_use_use_police"	,"has_secondary_use_other"]
train_values_subset = train_values[selected_features]

train_values_subset["age"] = winsorize(train_values_subset["age"],(0, 0.05))
train_values_subset["area_percentage"] = winsorize(train_values_subset["area_percentage"],(0, 0.055))
train_values_subset["height_percentage"] = winsorize(train_values_subset["height_percentage"],(0, 0.04))



train_values_subset["floors/area_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["area_percentage"]
train_values_subset["floors/height_p"] = train_values_subset["count_floors_pre_eq"]/train_values_subset["height_percentage"]
train_values_subset["height_p/area_p"] = train_values_subset["height_percentage"]/train_values_subset["area_percentage"]
train_values_subset["age*floors"] = train_values_subset["age"]*train_values_subset["count_floors_pre_eq"]


train_values_subset["doesnt_have_secondary_use"] = 1-train_values_subset["has_secondary_use"]
del train_values_subset["has_secondary_use"]
train_values_subset["has_secondary_use_other"]  = train_values_subset["has_secondary_use_other"]  |  train_values_subset["has_secondary_use_rental"] |  train_values_subset["has_secondary_use_use_police"] |  train_values_subset["has_secondary_use_institution"]|  train_values_subset["has_secondary_use_school"]|  train_values_subset["has_secondary_use_industry"]|  train_values_subset["has_secondary_use_health_post"]|  train_values_subset["has_secondary_use_gov_office"]
del train_values_subset["has_secondary_use_rental"]
del train_values_subset["has_secondary_use_institution"]
del train_values_subset["has_secondary_use_industry"]
del train_values_subset["has_secondary_use_health_post"]
del train_values_subset["has_secondary_use_gov_office"]
del train_values_subset["has_secondary_use_use_police"]
del train_values_subset["has_secondary_use_school"]

train_values_subset = pd.get_dummies(train_values_subset)
train_values_subset.head()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(train_values_subset,train_labels,test_size=0.2,train_size=0.8)

HGS.fit(X_train,y_train)

HGS.best_params_

preds = HGS.predict(X_test)

from sklearn.metrics import f1_score


f1_score(y_test, preds, average='micro')

test_values_subset = test_values[selected_features]


test_values_subset["age"] = winsorize(test_values_subset["age"],(0, 0.05))
test_values_subset["area_percentage"] = winsorize(test_values_subset["area_percentage"],(0, 0.055))
test_values_subset["height_percentage"] = winsorize(test_values_subset["height_percentage"],(0, 0.04))


test_values_subset["floors/area_p"] = test_values_subset["count_floors_pre_eq"]/test_values_subset["area_percentage"]
test_values_subset["floors/height_p"] = test_values_subset["count_floors_pre_eq"]/test_values_subset["height_percentage"]
test_values_subset["height_p/area_p"] = test_values_subset["height_percentage"]/test_values_subset["area_percentage"]
test_values_subset["age*floors"] = test_values_subset["age"]*test_values_subset["count_floors_pre_eq"]

test_values_subset["doesnt_have_secondary_use"] = 1-test_values_subset["has_secondary_use"]
del test_values_subset["has_secondary_use"]
test_values_subset["has_secondary_use_other"]  = test_values_subset["has_secondary_use_other"]  |  test_values_subset["has_secondary_use_rental"] |  test_values_subset["has_secondary_use_use_police"] |  test_values_subset["has_secondary_use_institution"]|  test_values_subset["has_secondary_use_school"]|  test_values_subset["has_secondary_use_industry"]|  test_values_subset["has_secondary_use_health_post"]|  test_values_subset["has_secondary_use_gov_office"]
del test_values_subset["has_secondary_use_rental"]
del test_values_subset["has_secondary_use_institution"]
del test_values_subset["has_secondary_use_industry"]
del test_values_subset["has_secondary_use_health_post"]
del test_values_subset["has_secondary_use_gov_office"]
del test_values_subset["has_secondary_use_use_police"]
del test_values_subset["has_secondary_use_school"]

test_values_subset = pd.get_dummies(test_values_subset)


predictions = HGS.predict(test_values_subset)

submission_format = pd.read_csv('submission_format.csv', encoding='latin1',sep=',', index_col='building_id')
my_submission = pd.DataFrame(data=predictions,
                             columns=submission_format.columns,
                             index=submission_format.index)

my_submission.head()

my_submission.to_csv('submission.csv')
!head submission.csv

from google.colab import files
files.download("submission.csv")