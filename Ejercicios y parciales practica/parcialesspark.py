# -*- coding: utf-8 -*-
"""ParcialesSpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WjIwRY5PmOPa6JT-eEK6e2ZTrptrwltL
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt update
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext
from pyspark.sql import SQLContext

spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

"""###Spark 2020 2C 1

Tenemos un RDD con información de recetas:
(ID_Receta, Nombre, Categoría)
Y otro RDD con los ingredientes de cada receta:
(ID_Receta, Ingrediente, Cantidad_Kg)
Queremos obtener:

a) Listar todos los ingredientes que aparecen en alguna receta que usa "pollo" indicando en
cuantas recetas el ingrediente y pollo aparecen juntos. El formato de salida es (ingrediente,
cantidad de recetas en que aparece junto con pollo). Por ejemplo, la papa aparece en 10
recetas con pollo, por lo que tendríamos (papa, 10). (50 pts)

b) Queremos obtener todos los nombres de recetas Mediterráneas que no tengan ni papa ni
pollo entre sus ingredientes.(50 pts)
Resolver los puntos usando la API de RDDs de PySpark
"""

#(ID_Receta,Nombre,Categoria)
recetas = [(1,"Pollo al horno","Mediterranea"),
           (2,"Pollo con papa","Mediterranea"),
           (3,"cesar ensalada","Griega"),
           (4,"arroz con pollo","Mediterranea"),
           (5,"Asado","Argentina"),
           (6,"Tofu","Mediterranea")
           ]

#(ID_Receta,ingredinte,cantidad_kg)
ingredientes = [(1,"pollo",2),
                (1,"arroz",2),
                (2,"pollo",2),
                (2,"papa",2),
                
                (3,"lechuga",2),
                (3,"pollo",2),
                (3,"papa",2),
                (4,"pollo",2),
                (4,"arroz",2),
                (5,"carne",2),
                (6,"arroz",2),
                (6,"tofu",2),
                ]


recetasRdd = sc.parallelize(recetas)
ingredientesRdd = sc.parallelize(ingredientes)

"""##a)"""

recetas_con_pollo = ingredientesRdd.filter(lambda x: x[1]=="pollo" ).map(lambda x: (x[0],x[1]) ).join(ingredientesRdd).map(lambda x: (x[1][1],1) ).filter(lambda x: x[0]!="pollo").reduceByKey(lambda x,y: x+y)
recetas_con_pollo.collect()

"""##b)"""

recetas_mediterraneas = recetasRdd.filter(lambda x: x[2]=="Mediterranea")
ingredientes_mediterraneos = recetas_mediterraneas.join(ingredientesRdd).map(lambda x: (x[1][0],x[1][1]) ).groupByKey().mapValues(list).filter(lambda x: ("pollo" or "papa") not in x[1] ).map(lambda x: x[0])
ingredientes_mediterraneos.collect()

"""###Primer Cuatrimestre de 2020

Dado los acontecimientos en USA, deseamos obtener datos que nos den mayor información sobre las muertes de personas de raza negra por parte de oficiales de policía.
Para ello, tenemos un csv con información sobre las muertes por parte de oficiales de policía en USA desde 2015 hasta 2017: 

(name, date, race, city, state)

Y otro csv con información sobre el porcentaje de cada raza en las ciudades de USA: 

(state, city, share_white, share_black, share_native_american, share_asian, share_hispanic)

Se pide:

a) Obtener el estado con mayor porcentaje de muertes de personas de raza negra teniendo en cuenta la cantidad total de muertes por parte de oficiales en ese estado. (10 pts)

b) Obtener los 10 estados con mayor diferencia entre el porcentaje de muertes y el porcentaje de gente de raza negra en ese estado. Para ello, considerar el porcentaje de raza de un estado como el promedio de los valores de sus ciudades.  Por ejemplo si en Texas el porcentaje de muertes de personas de raza negra por parte de la policía es del 36% y el promedio de share_black para Texas es 24% la diferencia es 0.12. (15 pts)

Resolver ambos puntos usando la API de RDDs de PySpark. `

"""

#(name, date, race, city, state)
muertes_por_policia = [("N1","2015-05-24","black","C1","E1"),
                       ("N2","2015-05-24","black","C2","E1"),
                       ("N3","2015-05-24","native american","C3","E2"),
                       ("N4","2015-05-24","black","C4","E3"),
                      ("N4","2015-05-24","asian","C4","E3"),
                       ("N5","2015-05-24","black","C5","E4"),
                       ("N6","2015-05-24","asian","C7","E4"),
                       ("N7","2015-05-24","black","C7","E4"),
                       ("N8","2015-05-24","hispanic","C8","E5")]
#(state, city, share_white, share_black, share_native_american, share_asian, share_hispanic)
ciudades = [("E1","C1",0.1,0.2,0.3,0.1,0.3),
            ("E1","C2",0.1,0.4,0.3,0.1,0.3),
            ("E2","C3",0.5,0.2,0.4,0.1,0.3),
            ("E3","C4",0.2,0.2,0.3,0.4,0.3),
            ("E4","C5",0.1,0.2,0.3,0.1,0.4),
            ("E4","C6",0.1,0.2,0.3,0.2,0.3),
            ("E4","C7",0.1,0.2,0.2,0.1,0.3),
            ("E5","C8",0.2,0.2,0.3,0.1,0.3)]


muertes_por_policiaRdd = sc.parallelize(muertes_por_policia)
ciudadesRdd = sc.parallelize(ciudades)

"""###a)"""

muertes_policiales_estado = muertes_por_policiaRdd.map(lambda x: (x[4],  (1 if x[2]=="black" else 0,1 ) )     ).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])).map(lambda x: (x[0],(x[1][0]/x[1][1])*100)      ).cache()
muertes_policiales_estado.collect()

muertes_policiales_estado.reduce(lambda x,y: x if x[1]>y[1] else y)

"""###b)"""

ciudades_black = ciudadesRdd.map(lambda x: (x[0],(x[3],1)) ).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]) ).map(lambda x: (x[0], x[1][0]/x[1][1]) )
ciudades_black.collect()

import numpy as np
#tomo las 3 ciudades con mayor diferencia
diferencia = muertes_policiales_estado.join(ciudades_black).map(lambda x: (x[0],np.abs( x[1][0]-x[1][1]) ) )
diferencia.takeOrdered(3, key=lambda x: -x[1])

"""##Spark 2020-2C 

Se tiene un RDD con la información de libros:

(id_libro, nombre, género, autor)

y otro con las ventas de distintos ejemplares de esos libros.

(id_venta, id_libro, dia_venta, mes_venta, año_venta, hora_venta, precio)

a) Indicar el género con más ventas de agosto de 2020.

b) Para los libros de los 5 géneros más vendidos en agosto de 2020, indicar el nombre del libro que presenta mayor aumento en el número de ventas con respecto al mes anterior.


Resolver los siguientes puntos usando la API de RDDs de PySpark.
Para ambos casos, asumir que no hay “empates” al buscar el máximo (no hay más de un género
con máxima cantidad de ventas en agosto, ni más uno con máxima diferencia de ventas con el
mes anterior).
"""

#(id_libro, nombre, género, autor)

libros = [(1, "Crónicas marcianas", "ciencia ficción", "Ray Bradbury"),
(2, "La vuelta al mundo en ochenta días", "aventura"," Julio Verne"),
(3, "Metro 2033", "ciencia ficción"," dimitry")
]

#(id_venta, id_libro, dia_venta, mes_venta, año_venta, hora_venta, precio)

ventas = [(100, 1, 10, "julio", 2020, "13:00:24", 300),
(101, 1, 20, "julio", 2020, "15:04:00", 300),
(102, 2, 23, "julio", 2020, "16:01:01", 250),
(103, 1, 8, "agosto", 2020, "16:22:23", 300),
(104, 1, 12, "agosto", 2020, "17:00:00", 300),
(105, 1, 12, "agosto", 2020, "17:07:07", 300),
(106, 2, 18, "agosto", 2020, "11:02:00", 250),
(107, 2, 19, "agosto", 2020, "11:42:00", 250),
(108, 2, 22, "agosto", 2020, "18:33:00", 250),
(109, 3, 22, "agosto", 2020, "18:33:00", 200),
(110, 3, 22, "agosto", 2020, "18:33:00", 200)
]

librosRdd = sc.parallelize(libros)
ventasRdd = sc.parallelize(ventas)

"""###a)"""

libros_genero = librosRdd.map(lambda x: (x[0],x[2]) )
ventas_id = ventasRdd.filter(lambda x: x[3] == "agosto" and x[4]==2020).map(lambda x: (x[1],1))
ventas_genero = libros_genero.join(ventas_id).map(lambda x: (x[1][0],x[1][1]) ).reduceByKey(lambda x,y: x+y).cache()
ventas_genero.reduce(lambda x,y: x if x[1]>y[1] else y)

"""###b)"""

generos_mas_vendidos_agosto = sc.parallelize(ventas_genero.takeOrdered(2, key=lambda x: -x[1]))

libros_por_genero = librosRdd.map(lambda x: (x[2],x[0]) )
libros_por_genero_mas_vendido = libros_por_genero.join(generos_mas_vendidos_agosto).map(lambda x: (x[1][0],1) )
libros_por_genero_mas_vendido.collect()

ventas_id_meses_incremento = ventasRdd.map(lambda x: (x[1],(1,0) if x[3]=="agosto" else (0,1)) ).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])).map(lambda x: (x[0],x[1][0]-x[1][1])) 
ventas_id_meses_incremento.collect()

d = libros_por_genero_mas_vendido.join(ventas_id_meses_incremento).map(lambda x: (x[0],x[1][1] )).reduce(lambda x,y: x if x[1]>y[1] else y)
d

"""###ejercicio slack


los 10 libros más leidos y de esos los 3 resumenes mas leidos

resultado = (book_id, top_10_position,summary_id_1,summary_id_2,summary_id_3)
"""

#(summary_id,book_id,user_id,timestamp)
lecturas = [("s1","b1","U1","T1"),
            ("s2","b1","U1","T1"),
            ("s1","b1","U2","T1"),
            ("s3","b2","U2","T1"),
            ("s3","b2","U1","T1"),
            ("s4","b3","U3","T1"),
            ("s6","b3","U3","T1"),
            ("s7","b3","U2","T1"),
            ("s8","b4","U1","T1"),
            ("s9","b5","U5","T1"),
            ("s9","b5","U7","T1")

]


#(summary_id,book_id,creator_user_id,text)

summarys = [("s1","b1","U11","texto"),
            ("s2","b1","U112","texto"),
            ("s3","b2","U11","texto"),
            ("s4","b3","U11","texto"),
            ("s6","b3","U1s21","texto"),
            ("s7","b3","U11a","texto"),
            ("s8","b4","U1d1","texto"),
            ("s9","b5","U11","texto"),
            
]

lecturasRdd = sc.parallelize(lecturas)
summarysRdd = sc.parallelize(summarys)

#(book_id,summary_id)
summarysToJoinBook_id = summarysRdd.map(lambda x: (x[1],x[0]) )
summarysToJoinBook_id.collect()

lecturasLibrosMasConsultados = lecturasRdd.map(lambda x: (x[1],1) ).reduceByKey(lambda x,y: x+y)
lecturasLibrosMasConsultados = sc.parallelize(lecturasLibrosMasConsultados.takeOrdered(3, lambda x: -x[1]))
lecturasLibrosMasConsultados = lecturasLibrosMasConsultados
lecturasLibrosMasConsultados.collect() #(book_id,#veces consultado)

lecturasToJoinBookId = lecturasRdd.map(lambda x: (x[1],x[0]) )
lecturasDeResumenesDeLibrosMasConsultados = lecturasToJoinBookId.join(lecturasLibrosMasConsultados)
lecturasDeResumenesDeLibrosMasConsultados.collect()

lecturasDeResumenesDeLibrosMasConsultados=lecturasDeResumenesDeLibrosMasConsultados
lecturasDeResumenesDeLibrosMasConsultados.collect()

