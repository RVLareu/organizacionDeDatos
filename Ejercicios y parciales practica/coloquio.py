# -*- coding: utf-8 -*-
"""Coloquio.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15a40lXxN9u3ls8kaamXub4cyP3D9A8DK
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt update
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext
from pyspark.sql import SQLContext

spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

"""Tenemos en Spark un RDD formado por puntos en 6 dimensiones, el formato de los registros
es: (id, (x1,x2,x3,x4,x5,x6)). Los valores de cada dimensión ya están normalizados entre 1 y 5.
Queremos programar usando PySpark al algoritmo K-Means para encontrar 12 clusters. El
resultado final debe ser un RDD de la forma (id, cluster) indicando a que cluster pertenece cada
uno de los puntos del RDD original.
"""

puntos = [ (1,(2,3,5,2,5,3)),
          (2,(2,3,5,2,5,2)),
          (3,(2,3,3,2,5,3)),
          (4,(2,3,5,2,3,3)),
          (5,(2,3,5,3,5,3)),
          (6,(2,1,5,2,5,3)),
          (7,(2,3,5,1,5,3)),
          (8,(1,3,5,2,5,3)),
          (9,(5,3,5,2,5,3)),
          (10,(2,5,5,2,5,3)),
          (11,(2,4,5,2,5,3)),
          (12,(2,3,4,2,5,3)),
          (13,(2,3,5,4,5,3)),
          (14,(2,3,5,2,4,3))]

#k = 12

puntosRdd = sc.parallelize(puntos)
centroides = [(2,3,5,2,5,3),
          (2,3,5,2,5,2),
          (2,3,3,2,5,3),
          (2,3,5,2,3,3),
          (2,3,5,3,5,3),
          (2,1,5,2,5,3),
          (2,3,5,1,5,3),
          (1,3,5,2,5,3),
          (5,3,5,2,5,3),
          (2,5,5,2,5,3),
          (2,4,5,2,5,3),
          (2,3,4,2,5,5)]
import numpy as np

def distancia_entre_puntos(punto_a,punto_b):
  sum = 0
  for i in range(0,6):
    sum = sum + (punto_a[i]-punto_b[i])**2
  return  np.sqrt(sum)

def centroide_mas_cercano(punto):
  dist = 9999999999
  cluster = 0
  for i in range (len(centroides)):
    if distancia_entre_puntos(punto,centroides[i])< dist:
      dist = distancia_entre_puntos(punto,centroides[i])
      cluster = i
  return cluster+1

def dividir_punto(punto,divisor):
  for i in range(0,6):
    punto[i]=punto[i]/divisor
  return punto

def recalcular_centroides(centroide,punto):
  centroides[centroide]=punto

puntosRdd = puntosRdd.map(lambda x:   (x[0],(x[1],centroide_mas_cercano(x[1]))    )    )
puntosRdd.collect()

punto_centroide = puntosRdd.map(lambda x: (x[1][1],(x[1][0],1))).reduceByKey(lambda x,y: ((x[0][0]+y[0][0],x[0][1]+y[0][1],x[0][2]+y[0][2],x[0][3]+y[0][3],x[0][4]+y[0][4],x[0][5]+y[0][5]),x[1]+y[1]) ).map(lambda x:  (x[0], recalcular_centroides(x[0],x[1][0][0]/x[1][1],x[1][0][1]/x[1][1],x[1][0][2]/x[1][1],x[1][0][3]/x[1][1],x[1][0][4]/x[1][1],x[1][0][5]/x[1][1]),  (x[1][0][0]/x[1][1],x[1][0][1]/x[1][1],x[1][0][2]/x[1][1],x[1][0][3]/x[1][1],x[1][0][4]/x[1][1],x[1][0][5]/x[1][1])   )    )

centroides

"""Un sitio de venta de libros registra información sobre los libros disponibles para la venta, sus
autores, usuarios del sitio y relaciones entre los mismos. Un usuario puede “seguir” a un autor
para enterarse de nuevos libros publicados por el mismo. Los usuarios escriben reviews de los
libros. Los usuarios pueden seguir a otros usuarios cuyos reviews les resulten interesantes.

1) Usando el modelo del punto 1. Diseñe un algoritmo para sugerir a un usuario que libros
pueden interesarles. Explique de que forma manejaría los libros que por ser nuevos han sido
leidos por muy pocos usuarios y tienen muy pocos reviews.

2) Usando el modelo del punto 2. Diseñe un algoritmo para sugerir a un usuario que autores
seguir.
"""

