# -*- coding: utf-8 -*-
"""Spark-BigMac.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yEi-qFtEfVBUJvdFQzqFda0kE50Q1_6f

# Instalamos e importamos librerías
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt update
!apt install openjdk-8-jdk-headless -qq
#!apt install default-jre
#!apt install default-jdk
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext
from pyspark.sql import SQLContext
import pandas as pd

"""# Autenticamos con Google Drive"""

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

"""# Bajamos archivo con los datos del índice BigMac"""

downloaded = drive.CreateFile({'id':"1h9Lrqmc4uAkXeC0DJAg3f4VlJLut5C7S"})
downloaded.GetContentFile('bigmac.csv')

"""# Creamos el Spark Context"""

spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

"""## Leemos CSV"""

sqlContext = SQLContext(sc)
df = sqlContext.read.csv('bigmac.csv', header=True, inferSchema=True)
rdd = df.rdd

"""## Cantidad de registros"""

rdd.count()

"""## Qué datos tiene cada registro"""

rdd.take(5)

"""## Qué país y cuando tuvo el mayor valor en USD"""

rdd = rdd.filter(lambda x: x.dollar_ex != 0).cache()

rdd.reduce(lambda x,y: x if x.local_price / x.dollar_ex > y.local_price / y.dollar_ex else y)

rdd.filter(lambda x: x.iso_a3 != 'VEN').reduce(lambda x,y: x if x.local_price / x.dollar_ex > y.local_price / y.dollar_ex else y)

"""## Y el menor?"""

rdd.reduce(lambda x,y: x if x.local_price / x.dollar_ex < y.local_price / y.dollar_ex else y)

"""## Precios de Argentina"""

rdd.filter(lambda x: x.iso_a3 == 'ARG').collect()

"""### En USD"""

arg = rdd.filter(lambda x: x.iso_a3 == 'ARG')

arg_usd = arg.map(lambda x: (x.date, x.local_price/x.dollar_ex)).cache()

arg_usd.collect()

"""### Mayor"""

arg_usd.takeOrdered(3, lambda x: -x[1])

"""### Menor"""

arg_usd.takeOrdered(3, lambda x: x[1])

"""### 5 mayores"""



"""### 5 menores"""



"""### Valor promedio"""

arg_usd.map(lambda x: x[1]).reduce(lambda x,y: x+y) / arg_usd.count()

"""## Valores en USA"""

rdd.filter(lambda x: x.iso_a3 == 'USA').map(lambda x: (x.date, x.local_price)).collect()

"""## 10 más caros"""

rdd.map(lambda x: (x.name, (x.local_price / x.dollar_ex, 1))).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\
   .map(lambda x: (x[0],x[1][0]/x[1][1]))\
   .takeOrdered(10, lambda x:-x[1])

rdd.map(lambda x: (x.name, (x.local_price / x.dollar_ex, 1))).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\
   .takeOrdered(10, lambda x:-x[1][0]/x[1][1])

"""## 10 más baratos"""

