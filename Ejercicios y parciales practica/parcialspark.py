# -*- coding: utf-8 -*-
"""ParcialSpark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ptyPjoVyAVc_jbCZd8P9SojGZkgnK8G0
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt update
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext
from pyspark.sql import SQLContext

spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

# (fecha, dni, id_localidad, resultado)
#fecha: YYYY-MM-DD (primer semestre mes 01,02,03,04,05,06)
tests = [
         ("2021-03-01",123,1,1),
         ("2021-03-01",123,1,1),
         ("2021-03-01",122,1,0),
         ("2021-03-01",122,2,1),
         ("2021-02-01",142,3,0),
         ("2021-01-01",172,4,1),
         ("2021-01-01",172,4,0),
         ("2021-01-01",192,4,1),
         ("2021-01-01",192,5,0),
         ("2021-02-01",222,7,0),
         ("2021-02-01",222,7,1),
         ("2021-01-01",222,8,1),
         ("2021-04-01",222,8,1),
         ("2021-04-01",222,9,0),
         ("2021-04-01",222,9,0),
         ("2020-07-01",222,9,0)

]
# (id_localidad, nombre, provincia)

localidades = [
               (1,"nombre1","Cordoba"),
               (2,"nombre2","Cordoba"),
               (3,"nombre3","Cordoba"),

               (4,"nombre4","Formosa"),
               (5,"nombre5","Formosa"),

               (6,"nombre6","Santa fe"),
               (7,"nombre7","Santa fe"),
               (8,"nombre8","Santa fe"),
               (9,"nombre9","Santa fe"),

]

testsRdd = sc.parallelize(tests)
localidadesRdd = sc.parallelize(localidades)

localidad_resultado = testsRdd.filter(lambda x: x[0].split("-")[0]=="2021" and x[0].split("-")[1] in ["01","02","03","04","05","06"]).map(lambda x:    (x[2],(1,1)) if x[3] == 1 else (x[2],(0,1))     )

localidades_casos = localidad_resultado.join(localidadesRdd).map(lambda x: (x[1][1],x[1][0])).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])).filter(lambda x: x[1][1]>2)

#.map(lambda x: (x[0],x[1][0]/x[1][1])      )
localidades_casos.collect()

"""Indicar la provincia con mayor porcentaje de tests positivos en el primer semestre de
2021, teniendo en cuenta sólo las provincias con al menos 100 tests en dicho trimestre.

"""

#tests del primer semestre
testsSemestrePorLocalidad = testsRdd.filter(lambda x: x[0].split("-")[1] in ["01","02","03","04","05","06"] and x[0].split("-")[0] == "2021")\
                              .map(lambda x: (x[2],(1,1) if x[3] == 1 else (0,1))).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))
#(id_localidad),(positivos,#tests totales)

localidadesTojoin = localidadesRdd.map(lambda x: (x[0],x[2]))

# filtro por 4 tests por el tamaño del set de datos, iria 100
testsSemestrePorLocalidad.join(localidadesTojoin).map(lambda x: (x[1][1],x[1][0])).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\
                  .filter(lambda x: x[1][1]>4).map(lambda x: (x[0],x[1][0]/x[1][1]) ).reduce(lambda x,y: x if x[1]>y[1] else y)[0]

"""Indicar cantidad de localidades por rango de porcentaje de tests positivos en el primer
semestre de 2021 (utilizando rangos de 10%). El resultado debería tener la siguiente
estructura:
"""

#los intervalos los tomo:  
#  [0,10%) -> 0
#  [10%,20%) -> 1
#  [20%,30) -> 2
#  [30%,40) -> 3
#  [40%,50) -> 4
#  [50%,60) -> 5
#  [60%,70) -> 6
#  [70%,80) -> 7
#  [80%,90) -> 8
# (90,100] -> 9

# => estructura:  (rango inferior,cantidad de localidades)
#el if es para que el caso 100% quede en el bucket que le corresponde (el 9)
testsSemestrePorLocalidadPositividad = testsSemestrePorLocalidad.map(lambda x: (int(x[1][0]/x[1][1] * 10),1) if int(x[1][0]/x[1][1] * 10) != 10 else (9,1) )\
                                        .reduceByKey(lambda x,y: x+y).sortByKey()