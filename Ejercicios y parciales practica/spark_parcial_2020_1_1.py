# -*- coding: utf-8 -*-
"""Spark-Parcial-2020-1-1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J6QYI82ZPor0O0qHpaucobRrnj-PksqA

# Instalamos e importamos librerías
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext
from pyspark.sql import SQLContext
import pandas as pd

"""# Autenticamos con Google Drive"""

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

"""## Enunciado

Dado los acontecimientos en USA, deseamos obtener datos que nos den mayor información sobre las muertes de personas de raza negra por parte de oficiales de policía.

Para ello, tenemos un csv con información sobre las muertes por parte de oficiales de policía en USA desde 2015 hasta 2017: 

(name, date, race, city, state)

Y otro csv con información sobre el porcentaje de cada raza en las ciudades de USA: 

(state, city, share_white, share_black, share_native_american, share_asian, share_hispanic)

Se pide:

* a) Obtener el estado con mayor porcentaje de muertes de personas de raza negra teniendo en cuenta la cantidad total de muertes por parte de oficiales en ese estado. (10 pts)
* b) Obtener los 10 estados con mayor diferencia entre el porcentaje de muertes y el porcentaje de gente de raza negra en ese estado. Para ello, considerar el porcentaje de raza de un estado como el promedio de los valores de sus ciudades.  Por ejemplo si en Texas el porcentaje de muertes de personas de raza negra por parte de la policía es del 36% y el promedio de share_black para Texas es 24% la diferencia es 0.12. (15 pts)

Resolver ambos puntos usando la API de RDDs de PySpark. `

# Bajamos archivos
"""

downloaded = drive.CreateFile({'id':"1Qhsm9vtr-IN5UyRMEANR2brS71DY3neS"})
downloaded.GetContentFile('killing.csv')

downloaded = drive.CreateFile({'id':"1lWWUd2v9mIrcohl5qAb_kU3FP9jBCLd7"})
downloaded.GetContentFile('race.csv')

"""# Creamos el Spark Context y el SQL Context"""

# create the Spark Session
spark = SparkSession.builder.getOrCreate()

# create the Spark Context
sc = spark.sparkContext

sqlContext = SQLContext(sc)

"""# Leemos los datos

Fuente de datos e información sobre los mismos: https://www.kaggle.com/kwullum/fatal-police-shootings-in-the-us
"""

killing = sqlContext.read.csv('killing.csv', header=True).rdd.cache()
race = sqlContext.read.csv('race.csv', header=True).rdd.cache()

killing.first()

killing.count()

killing.map(lambda x: x.race).distinct().collect()

killing.map(lambda x: (x.race,1)).reduceByKey(lambda x,y:x+y).collect()

race.count()

race.first()

black_killing = killing.map(lambda x: (x.state, (1, 1 if x.race == 'B' else 0))).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])).map(lambda x: (x[0],float(x[1][1])*100/x[1][0])).cache()

black_killing.take(10)

black_killing.reduce(lambda x,y: x if x[1] > y[1] else y)

def to_float(string):
    try:
        return float(string)
    except:
        return 0

race_by_state = race.map(lambda x: (x[0],(1,to_float(x.share_black)))).reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1])).map(lambda x: (x[0],float(x[1][1])/x[1][0]))

race_by_state.take(10)

black_killing.join(race_by_state).map(lambda x: (x[0],(x[1][0]-x[1][1]))).takeOrdered(10, lambda x:-x[1])

